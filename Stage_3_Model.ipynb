{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stage-3 Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZh_w_Z4WK40"
      },
      "source": [
        "Modeling\n",
        "\n",
        "The model that we created is a modified version of the popular image segmentation model called the U-Net. With U-net as the baseline model for our segmentation task, we wanted to study about the latest techniques being used for image segmentation and apply those to the U-Net.\n",
        "\n",
        "Model description\n",
        "\n",
        "Just like the U-net, we have skip connections from the encoder side to the decoder side. The part that we added is an Atrous Spatial Pyramid Pooling(ASPP) block at the end of the encoder side. The new block is taken from Google's DeepLabv3. We perform multiple atrous convolutions on the same map and concatenate all those outputs. This gives us an feature map with a hierarchy of features which would better help segmentating the image while performing transpose convolutions on the decoder side.\n",
        "\n",
        "Encoder \n",
        "\n",
        "On the encoder side we have 4 convolutional blocks and a ASPP block. Each convolution block contains 2 convolutional layers with a dropout layer between them and a max pooling block at the end. With every convolutional block we increase the number of filters in the convolutional layers\n",
        "\n",
        "Decoder\n",
        "\n",
        "On the decoder side, we have 2 upscaling blocks. Each of these upsacling blocks have a transpose convolution layer and 2 convolutional layers with a dropout layer between them. At the end, after upscaling the image back to its input size, we have a convolutional layer with a 1x1 filter size and a sigmoid activation function.\n",
        "\n",
        "Hyper-parameter information\n",
        "\n",
        "For this application, we used a loss function that is a combination of the binary cross entropy loss and the dice loss, derived from the dice coefficient.\n",
        "The optimizer we used is ADAM. We trained this model for 100 epochs with a batch size of 32.\n",
        "\n",
        "Experimentation\n",
        "\n",
        "We used Google Colab to run our experimentations. The dataset we used is called the PFCN dataset, referencing the paper “Xiaoyong Shen, Xin Tao, Hongyun Gao, Chao Zhou, Jiaya Jia. Deep Automatic Portrait Matting. European Conference on Computer Vision (ECCV),2016”. The images in this dataset have large structure variation for background and foreground images. Finally, 2,000 images \n",
        "with high quality mattes are collected. In these 2000 images, 1700 images are used for training and 300 images are used for testing. And out of the 1700 images, we set 0.15 as the validation set.\n",
        "\n",
        "The main framework for training and modeling is TensorFlow 2.0. We started with a smaller and similar implementation of the U-Net architecture and researched the current techniques being used of image segmentation. We want to build a image segmentation model for blurring the background in a portrait that is small and simple like the U-Net.\n",
        "\n",
        "Literature Review\n",
        "\n",
        "U-Net\n",
        "Convolutional Networks for Biomedical Image Segmentation.\n",
        "https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n",
        "\n",
        "Google DeepLab v3\n",
        "https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqh58jap1SUv",
        "outputId": "f103830c-fb48-42cc-c6fd-652171962650"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HR_H0FThD3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a9b1dc-7694-4d71-9828-fb2848f8ffd9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "  \n",
        "def bce_dice_loss(y_true, y_predict):\n",
        "    return binary_crossentropy(y_true, y_predict) + (1-dice_coef(y_true, y_predict))\n",
        "\n",
        "def  dice_loss(y_true, y_predict):\n",
        "    return (1-dice_coef(y_true, y_predict))\n",
        "\n",
        "model = models.load_model('/content/drive/My Drive/CVProject/unetv2', custom_objects={'dice_coef': dice_coef, 'bce_dice_loss' : bce_dice_loss})\n",
        "\n",
        "frame = cv2.imread('/content/drive/My Drive/CVProject/test13.jpg')        \n",
        "\n",
        "height, width, channels = frame.shape\n",
        "frame = cv2.resize(frame, (600,800))\n",
        "frame1 = frame[np.newaxis, :, :, :]\n",
        "\n",
        "mask = model(frame1)\n",
        "mask = np.array(mask)\n",
        "mask = mask[0][:,:,0]\n",
        "\n",
        "full_mask = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
        "full_mask[:,:,0] = mask\n",
        "full_mask[:,:,1] = mask\n",
        "full_mask[:,:,2] = mask\n",
        "\n",
        "frame = frame.astype(float)\n",
        "full_mask = full_mask.astype(float)\n",
        "\n",
        "subject = cv2.multiply(full_mask, frame)\n",
        "bg = cv2.multiply(1-full_mask, frame)\n",
        "bg = cv2.GaussianBlur(bg, (7,7), 2)\n",
        "\n",
        "final = cv2.add(bg, subject)\n",
        "final = final.astype('uint8')\n",
        "final = cv2.resize(final, (width, height))\n",
        "cv2.imwrite('test13_res.jpg', final)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSus9oabhAHx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIUL5Hirf36O"
      },
      "source": [
        "Results\n",
        "\n",
        "Metrics used \n",
        "\n",
        "We used the dice co-efficient that calculates the overlap between the predicted output mask and the actual output mask. On our test set, we got a score for 93.56 after training for 100 epochs.\n",
        "\n",
        "Visual results\n",
        "\n",
        "input:\n",
        "https://drive.google.com/file/d/136G5xOcE8hM6V4YZP__AMfLY2Q6fxbHM/view?usp=sharing\n",
        "\n",
        "\n",
        "output:\n",
        "https://drive.google.com/file/d/1eEFkYzQO4gg20TObutE00qX-2goabQIa/view?usp=sharing\n",
        "\n",
        "We believe is a good progress but isn't the latest step. We plan on exploring and trying different loss functions and making changes to the model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdATGINU1Lzy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}